{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf66233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec5004e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Counts: {'looking_down': 49, 'looking_left': 24, 'looking_right': 30, 'looking_straight': 24, 'looking_up': 49, 'multiple_faces': 12}\n",
      "Balanced dataset created and saved. Total samples: 3000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# ========================\n",
    "# Dataset path\n",
    "# ========================\n",
    "data_dir = r\"C:\\Dataset_ClassWise\"\n",
    "\n",
    "# Load ImageFolder dataset\n",
    "full_dataset = datasets.ImageFolder(root=data_dir)\n",
    "\n",
    "# ========================\n",
    "# Transforms\n",
    "# ========================\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224))\n",
    "])\n",
    "\n",
    "# Separate augmentation for straight-looking faces\n",
    "def get_augment_transform(label):\n",
    "    if label == \"looking_straight\":\n",
    "        # Minimal augmentation for straight-looking\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "        ])\n",
    "    else:\n",
    "        # Aggressive augmentation for other classes\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        ])\n",
    "\n",
    "to_tensor_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],\n",
    "                         [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# ========================\n",
    "# Balanced Dataset Class\n",
    "# ========================\n",
    "class BalancedDataset(Dataset):\n",
    "    def __init__(self, dataset, target_size=500):\n",
    "        self.dataset = dataset\n",
    "        self.target_size = target_size\n",
    "        self.indices = []\n",
    "        self._balance_dataset()\n",
    "\n",
    "    def _balance_dataset(self):\n",
    "        # Count original samples\n",
    "        class_counts = {cls: 0 for cls in self.dataset.classes}\n",
    "        for _, label in self.dataset.samples:\n",
    "            class_counts[self.dataset.classes[label]] += 1\n",
    "        print(\"Original Counts:\", class_counts)\n",
    "\n",
    "        # Collect indices by class\n",
    "        class_indices = {cls: [] for cls in self.dataset.classes}\n",
    "        for idx, (_, label) in enumerate(self.dataset.samples):\n",
    "            class_indices[self.dataset.classes[label]].append(idx)\n",
    "\n",
    "        balanced_indices = []\n",
    "        for cls in self.dataset.classes:\n",
    "            idxs = class_indices[cls]\n",
    "            if len(idxs) >= self.target_size:\n",
    "                # Undersample\n",
    "                balanced_indices.extend(random.sample(idxs, self.target_size))\n",
    "            else:\n",
    "                # Oversample\n",
    "                balanced_indices.extend(idxs)\n",
    "                while len([i for i in balanced_indices if i in idxs]) < self.target_size:\n",
    "                    balanced_indices.append(random.choice(idxs))\n",
    "        self.indices = balanced_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.dataset.samples[self.indices[idx]]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = base_transform(img)\n",
    "        # Apply class-specific augmentation if oversampled\n",
    "        if self.indices.count(self.indices[idx]) > 1:\n",
    "            img = get_augment_transform(self.dataset.classes[label])(img)\n",
    "        img = to_tensor_transform(img)\n",
    "        return img, label\n",
    "\n",
    "# ========================\n",
    "# Create & save balanced dataset\n",
    "# ========================\n",
    "balanced_dataset = BalancedDataset(full_dataset, target_size=500)\n",
    "torch.save({\n",
    "    'indices': balanced_dataset.indices,\n",
    "    'samples': full_dataset.samples,\n",
    "    'classes': full_dataset.classes\n",
    "}, r\"C:\\Dataset_ClassWise\\balanced_dataset_new.pt\")  # different file name\n",
    "\n",
    "print(\"Balanced dataset created and saved. Total samples:\", len(balanced_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "971b6a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Dataset Size: 3000\n",
      "Classes: ['looking_down', 'looking_left', 'looking_right', 'looking_straight', 'looking_up', 'multiple_faces']\n",
      "Train samples: 2400\n",
      "Validation samples: 600\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# ========================\n",
    "# Load saved balanced dataset\n",
    "# ========================\n",
    "data = torch.load(r\"C:\\Dataset_ClassWise\\balanced_dataset_new.pt\")\n",
    "\n",
    "# ========================\n",
    "# Define transforms\n",
    "# ========================\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224))\n",
    "])\n",
    "\n",
    "to_tensor_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],\n",
    "                         [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# Class-specific augmentation function\n",
    "def get_augment_transform(label):\n",
    "    if label == \"looking_straight\":\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        ])\n",
    "\n",
    "# ========================\n",
    "# Dataset Wrapper using saved indices\n",
    "# ========================\n",
    "class SavedBalancedDataset(Dataset):\n",
    "    def __init__(self, saved_data):\n",
    "        self.indices = saved_data['indices']\n",
    "        self.samples = saved_data['samples']\n",
    "        self.classes = saved_data['classes']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[self.indices[idx]]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = base_transform(img)\n",
    "        img = get_augment_transform(self.classes[label])(img)  # class-specific augmentation\n",
    "        img = to_tensor_transform(img)\n",
    "        return img, label\n",
    "\n",
    "# ========================\n",
    "# Load dataset\n",
    "# ========================\n",
    "balanced_dataset = SavedBalancedDataset(data)\n",
    "print(\"Balanced Dataset Size:\", len(balanced_dataset))\n",
    "print(\"Classes:\", balanced_dataset.classes)\n",
    "\n",
    "# ========================\n",
    "# Train/Validation Split\n",
    "# ========================\n",
    "labels = [balanced_dataset.samples[idx][1] for idx in balanced_dataset.indices]\n",
    "\n",
    "indices = list(range(len(balanced_dataset)))\n",
    "train_idx, val_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "train_dataset = Subset(balanced_dataset, train_idx)\n",
    "val_dataset = Subset(balanced_dataset, val_idx)\n",
    "\n",
    "print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Validation samples:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d388e93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch [1/10] Train Loss: 1.1323 Train Acc: 0.6683 Val Loss: 0.6245 Val Acc: 0.8533 Time: 344.3s\n",
      "Epoch [2/10] Train Loss: 0.3730 Train Acc: 0.9150 Val Loss: 0.1678 Val Acc: 0.9567 Time: 313.3s\n",
      "Epoch [3/10] Train Loss: 0.1221 Train Acc: 0.9675 Val Loss: 0.0776 Val Acc: 0.9767 Time: 312.8s\n",
      "Epoch [4/10] Train Loss: 0.0662 Train Acc: 0.9796 Val Loss: 0.0433 Val Acc: 0.9800 Time: 269.3s\n",
      "Epoch [5/10] Train Loss: 0.0539 Train Acc: 0.9817 Val Loss: 0.0340 Val Acc: 0.9833 Time: 284.8s\n",
      "Epoch [6/10] Train Loss: 0.0407 Train Acc: 0.9879 Val Loss: 0.0157 Val Acc: 0.9967 Time: 298.6s\n",
      "Epoch [7/10] Train Loss: 0.0372 Train Acc: 0.9850 Val Loss: 0.0217 Val Acc: 0.9867 Time: 301.7s\n",
      "Epoch [8/10] Train Loss: 0.0326 Train Acc: 0.9892 Val Loss: 0.0118 Val Acc: 0.9967 Time: 312.8s\n",
      "Epoch [9/10] Train Loss: 0.0278 Train Acc: 0.9925 Val Loss: 0.0182 Val Acc: 0.9900 Time: 297.9s\n",
      "Epoch [10/10] Train Loss: 0.0180 Train Acc: 0.9908 Val Loss: 0.0130 Val Acc: 0.9967 Time: 222.7s\n",
      "Training complete! Best validation accuracy: 0.9967\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import time\n",
    "\n",
    "# ========================\n",
    "# Device\n",
    "# ========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ========================\n",
    "# DataLoaders\n",
    "# ========================\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "# ========================\n",
    "# Load pretrained MobileNetV2\n",
    "# ========================\n",
    "model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "\n",
    "# Replace classifier for 6 classes\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_ftrs, 6)\n",
    "model = model.to(device)\n",
    "\n",
    "# ========================\n",
    "# Loss & optimizer\n",
    "# ========================\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ========================\n",
    "# Training Loop\n",
    "# ========================\n",
    "num_epochs = 10\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    val_loss /= total\n",
    "    val_acc = correct / total\n",
    "\n",
    "    # Save best model (new file)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), r\"C:\\Dataset_ClassWise\\mobilenetv2_best_new.pth\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} Train Acc: {train_acc:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f} Val Acc: {val_acc:.4f} \"\n",
    "          f\"Time: {end_time-start_time:.1f}s\")\n",
    "\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54eab649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded successfully!\n",
      "Predicted Eye Gaze Class: looking_right\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# ========================\n",
    "# Device\n",
    "# ========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ========================\n",
    "# Classes\n",
    "# ========================\n",
    "classes = ['looking_down', 'looking_left', 'looking_right', 'looking_straight', 'looking_up', 'multiple_faces']\n",
    "\n",
    "# ========================\n",
    "# Load trained model\n",
    "# ========================\n",
    "model = models.mobilenet_v2(weights=None)\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = torch.nn.Linear(num_ftrs, len(classes))\n",
    "model.load_state_dict(torch.load(r\"C:\\Dataset_ClassWise\\mobilenetv2_best_new.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# ========================\n",
    "# Preprocessing function\n",
    "# ========================\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# ========================\n",
    "# Load image\n",
    "# ========================\n",
    "img_path = r\"C:\\Dataset_ClassWise\\looking_right\\test.jpg\"  # replace with your test image path\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img_tensor = preprocess(img).unsqueeze(0).to(device)  # add batch dimension\n",
    "\n",
    "# ========================\n",
    "# Predict\n",
    "# ========================\n",
    "with torch.no_grad():\n",
    "    outputs = model(img_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    print(\"Predicted Eye Gaze Class:\", classes[predicted.item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00661d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded successfully!\n",
      "Starting video proctoring... Press 'q' to quit.\n",
      "Cheating detected at 12.61s! Reason: suspicious > 4.0s\n",
      "Gaze log saved to gaze_log.csv\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# ========================\n",
    "# Device\n",
    "# ========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ========================\n",
    "# Classes\n",
    "# ========================\n",
    "classes = ['looking_down', 'looking_left', 'looking_right', 'looking_straight', 'looking_up', 'multiple_faces']\n",
    "\n",
    "# ========================\n",
    "# Load model\n",
    "# ========================\n",
    "model = models.mobilenet_v2(weights=None)\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = torch.nn.Linear(num_ftrs, len(classes))\n",
    "model.load_state_dict(torch.load(r\"C:\\Dataset_ClassWise\\mobilenetv2_best_new.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# ========================\n",
    "# Preprocess function\n",
    "# ========================\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# ========================\n",
    "# Exam rules\n",
    "# ========================\n",
    "MAX_CHANCES = 3            # number of allowed violations\n",
    "MAX_CONTINUOUS = 4.0       # max seconds looking away continuously\n",
    "WINDOW_TIME = 20.0         # seconds interval to monitor violations\n",
    "\n",
    "# ========================\n",
    "# Start Video Capture\n",
    "# ========================\n",
    "cap = cv2.VideoCapture(0)  # change index if needed\n",
    "start_time = time.time()\n",
    "violation_count = 0\n",
    "continuous_away_start = None\n",
    "gaze_log = []\n",
    "\n",
    "print(\"Starting video proctoring... Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    timestamp = time.time() - start_time\n",
    "\n",
    "    # Convert frame to PIL and preprocess\n",
    "    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Predict gaze\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        gaze = classes[predicted.item()]\n",
    "\n",
    "    # Determine if suspicious\n",
    "    if gaze != \"looking_straight\":\n",
    "        if continuous_away_start is None:\n",
    "            continuous_away_start = timestamp\n",
    "        continuous_duration = timestamp - continuous_away_start\n",
    "        reason = \"looked away\"\n",
    "    else:\n",
    "        continuous_duration = 0\n",
    "        continuous_away_start = None\n",
    "        reason = \"normal\"\n",
    "\n",
    "    # Update violation count if continuous > MAX_CONTINUOUS\n",
    "    if continuous_duration > MAX_CONTINUOUS:\n",
    "        violation_count += 1\n",
    "        continuous_away_start = None  # reset\n",
    "        reason = f\"suspicious > {MAX_CONTINUOUS}s\"\n",
    "\n",
    "    # Append log\n",
    "    gaze_log.append({\n",
    "        \"Time (s)\": round(timestamp, 2),\n",
    "        \"Gaze\": gaze,\n",
    "        \"ContinuousAway(s)\": round(continuous_duration,2),\n",
    "        \"ViolationCount\": violation_count,\n",
    "        \"Reason\": reason\n",
    "    })\n",
    "\n",
    "    # Display frame with info\n",
    "    cv2.putText(frame, f\"Gaze: {gaze}\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "    cv2.putText(frame, f\"Violations: {violation_count}/{MAX_CHANCES}\", (10,70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "    cv2.imshow(\"Exam Proctoring\", frame)\n",
    "\n",
    "    # Terminate if violations exceed limit\n",
    "    if violation_count >= MAX_CHANCES:\n",
    "        print(f\"Cheating detected at {timestamp:.2f}s! Reason: {reason}\")\n",
    "        break\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# ========================\n",
    "# Save log to CSV\n",
    "# ========================\n",
    "log_df = pd.DataFrame(gaze_log)\n",
    "log_df.to_csv(r\"C:\\Dataset_ClassWise\\gaze_log.csv\", index=False)\n",
    "print(\"Gaze log saved to gaze_log.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eye_gaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
